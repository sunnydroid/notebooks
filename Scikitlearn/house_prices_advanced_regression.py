# dataquest tutorial here: https://www.dataquest.io/blog/kaggle-getting-started/
# competition/data: https://www.kaggle.com/c/house-prices-advanced-regression-techniques
# data exploration: https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python/notebook

########################
# Steps to building a model

# Acquire the data
# Explore the data
# Engineer and transform the features and the target variable
# Build a model

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
# import seaborn as sns
import matplotlib
# set renderer for osx
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt

# set plot style sheet to 'ggplot' and set rc parameters
plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10, 6)

#####################
# Acquire the data

train = pd.read_csv('datasets/housing_data/train.csv')
test = pd.read_csv('datasets/housing_data/test.csv')

# check the size of the data
print("train shape = ", train.shape)
print("test shape = ", test.shape)

# The difference in column size between train and test, 81 vs 80 is because test data doesn't have the outcome/target
# in this case, the sale price

# explore the training set using DataFrame.head() method
print(train.head())

# We want to predict the final price of each home. Looking at the data, we see features we expected, like YrSold
# (the year the home was last sold) and SalePrice. Others we might not have anticipated, such as LandSlope (the slope
# of the land the home is built upon) and RoofMatl (the materials used to construct the roof). Later, we'll have to make
# decisions about how we'll approach these and other features.

# We want to do some plotting during the exploration stage of our project, and we'll need to import that functionality
# into our environment as well. Plotting allows us to visualize the distribution of the data, check for outliers, and
# see other patterns that we might miss otherwise. We'll use Matplotlib, a popular visualization library.

######################
# Explore the dataset

# We can use Series.describe() to get more information. Lets get information about the target we are trying to predict
print(train.SalePrice.describe())

# Series.describe() gives you more information about any series. count displays the total number of rows in the series.
# For numerical data, Series.describe() also gives the mean, std, min and max values as well.
#
# The average sale price of a house in our dataset is close to $180,000, with most of the values falling within the
# $130,000 to $215,000 range.
#
# Next, we'll check for skewness, which is a measure of the shape of the distribution of values.

# When performing regression, sometimes it makes sense to log-transform the target variable when it is skewed. One
# reason for this is to improve the linearity of the data.

# Importantly, the predictions generated by the final model will also be log-transformed, so we'll need to convert these
# predictions back to their original form later.

# np.log() will transform the variable, and np.exp() will reverse the transformation.

# We use plt.hist() to plot a histogram of SalePrice. Notice that the distribution has a longer tail on the right.
# The distribution is positively skewed.

print("Sale price Skew is: ", train.SalePrice.skew())
plt.subplot(211)
plt.hist(train.SalePrice, color='blue')
# plt.show()

# Now we use np.log() to transform train.SalePric and calculate the skewness a second time, as well as re-plot the data.
# A value closer to 0 means that we have improved the skewness of the data. We can see visually that the data will more
# resembles a normal distribution.

target = np.log(train.SalePrice)
print("Sale price skew after log transformation is: ", target.skew())
plt.subplot(212)
plt.hist(target, color='green')
# plt.show()

# Now that we've transformed the target variable, let's consider our features. First, we'll check out the numerical
# features and make some plots. The .select_dtypes() method will return a subset of columns matching the specified data
# types.

numerical_features = train.select_dtypes(include=[np.number])
print("Numberic features: ", numerical_features.dtypes)

# The DataFrame.corr() method displays the correlation (or relationship) between the columns. We'll examine the
# correlations between the features and the target.

corr = numerical_features.corr();
print("Values most correlated with sale price : ")
print(corr['SalePrice'].sort_values(ascending=False)[:10])
print("Values least correlated with sale price : ")
print(corr['SalePrice'].sort_values(ascending=False)[-5:])

# f, ax = plt.subplots(figsize=(12,9))
# sns.heatmap(corr, vmax=.8, square=True)
# 'OverallQual' has the most correlation to the sale price
# Notice 'GarageCars' and 'GarageArea' have very similar correlation to the sale price. Intuitively garage cars is
# dependent on the garage size.

# Let's dig deeper on OverallQual. We can use the .unique() method to get the unique values.
print("Overall quality unique values: ", train.OverallQual.unique())

# We can create a pivot table to further investigate the relationship between OverallQual and SalePrice. The Pandas
# docs demonstrate how to accomplish this task. We set index='OverallQual' and values='SalePrice'. We chose to look at
# the median here.

# A pivot table can help quickly summarize the data and highlight the desired information. The usage of a pivot table
# is extremely broad and depends on the situation. The first question to ask is, "What am I seeking?" Here we are interested
# viewing the median sale prices at each value of 'OverallQual'

qualit_pivot_table = pd.pivot_table(train, index='OverallQual', values='SalePrice', aggfunc=np.median)
print("Quality pivot table for medial sale prices")
print(qualit_pivot_table)

# To help us visualize this pivot table more easily, we can create a bar plot using the Series.plot() method.
qualit_pivot_table.plot(kind='bar', color='blue')
plt.xlabel('Overall quality')
plt.ylabel('Median sale price')
plt.xticks(rotation=0)
# plt.show()

# Notice that the median sales price strictly increases as Overall Quality increases.

# Next, let's use plt.scatter() to generate some scatter plots and visualize the relationship between the Ground Living
# Area GrLivArea and SalePrice.

plt.scatter(x=train['GrLivArea'], y=target)
plt.xlabel('Above grade living area')
plt.ylabel('Sale price')
# plt.show()

# At first glance, we see that increases in living area correspond to increases in price. We will do the same for GarageArea.
plt.scatter(x=train['GarageArea'], y=target)
plt.xlabel('Garage  area')
plt.ylabel('Sale price')
# plt.show()

# Notice that there are many homes with 0 for Garage Area, indicating that they don't have a garage. We'll transform
# other features later to reflect this assumption. There are a few outliers as well. Outliers can affect a regression
# model by pulling our estimated regression line further away from the true population regression line. So, we'll
# remove those observations from our data. Removing outliers is an art and a science. There are many techniques for
# dealing with outliers.

# We will create a new dataframe with some outliers removed.
train = train[train['GarageArea'] < 1200]

# Lets take another look
plt.scatter(x=train['GarageArea'], y=np.log(train.SalePrice))
plt.xlim(-200,1600) # This forces the same scale as before
plt.ylabel('Sale Price')
plt.xlabel('Garage Area')
# plt.show()



# Handling null values. We will create a DataFrame to view the top null columns. Chaining together the
# train.isnull().sum() methods, we return a Series of the counts of the null values in each column.

nulls = pd.DataFrame(train.isnull().sum().sort_values(ascending=False)[:25])
nulls.columns = ['Null Count']
nulls.index.name = 'Feature'

print(nulls)

# The documentation can help us understand the missing values. In the case of PoolQC, the column refers to Pool Quality.
# Pool quality is NaN when PoolArea is 0, or there is no pool.
# We can find a similar relationship between many of the Garage-related columns.

# Let's take a look at one of the other columns, MiscFeature. We'll use the Series.unique() method to return a list of
# the unique values.

print ("Unique values of misc features are:", train.MiscFeature.unique())

# We can use the documentation to find out what these values indicate:

# MiscFeature: Miscellaneous feature not covered in other categories

# Elev Elevator
# Gar2 2nd Garage (if not described in garage section)
# Othr Other
# Shed Shed (over 100 SF)
# TenC Tennis Court
# NA   None

# These values describe whether or not the house has a shed over 100 sqft, a second garage, and so on. We might want to
# use this information later. It's important to gather domain knowledge in order to make the best decisions when dealing
# with missing data.

# Wrangling the non-numeric Features

categoricals = train.select_dtypes(exclude=[np.number])
print(categoricals.describe())

# The count column indicates the count of non-null observations, while unique counts the number of unique values.
# top is the most commonly occurring value, with the frequency of the top value shown by freq.

# For many of these features, we might want to use one-hot encoding to make use of the information for modeling.
# One-hot encoding is a technique which will transform categorical data into numbers so the model can understand whether
# or not a particular observation falls into one category or another.

##############################################################
# Engineer and transform the features and the target variable

# When transforming features, it's important to remember that any transformations that you've applied to the training
# data before fitting the model must be applied to the test data.

# Our model expects that the shape of the features from the train set match those from the test set. This means that
# any feature engineering that occurred while working on the train data should be applied again on the test set.
# To demonstrate how this works, consider the Street data, which indicates whether there is Gravel or Paved road access
# to the property.

print ("Original values of street type and count:")
print (train.Street.value_counts())

# In the Street column, the unique values are Pave and Grvl, which describe the type of road access to the property.
# In the training set, only 5 homes have gravel access. Our model needs numerical data, so we will use one-hot encoding
# to transform the data into a Boolean column.

# We create a new column called enc_street. The pd.get_dummies() method will handle this for us.
# As mentioned earlier, we need to do this on both the train and test data.

train['enc_street'] = pd.get_dummies(train.Street, drop_first=True)
test['enc_street'] = pd.get_dummies(test.Street, drop_first=True)

print("Encoded values of street type")
print(train.enc_street.value_counts())

# The values agree. We've engineered our first feature! Feature Engineering is the process of making features of the
# data suitable for use in machine learning and modelling. When we encoded the Street feature into a column of Boolean
# values, we engineered a feature.

# Let's try engineering another feature. We'll look at SaleCondition by constructing and plotting a pivot table, as we
# did above for OverallQual.

print(train.SaleCondition.describe())
sale_condition_pivot_table = pd.pivot_table(train, index='SaleCondition', values='SalePrice', aggfunc=np.median)
sale_condition_pivot_table.plot(kind='bar', color='blue')
plt.xlabel('Sale Condition')
plt.ylabel('Sale Price')
plt.xticks(rotation=0)
# plt.show()

# Notice that Partial has a significantly higher Median Sale Price than the others. We will encode this as a new feature.
# We select all of the houses where SaleCondition is equal to Patrial and assign the value 1, otherwise assign 0.

# We'll use a slightly different method for encoding than that we used for Street above.

def encode(x):
    return 1 if x == 'Partial' else 0

train['enc_sale_condition'] = train.SaleCondition.apply(encode)
test['enc_sale_condition'] = test.SaleCondition.apply(encode)

print("Encoded values of sale condition")
print(train.enc_sale_condition.describe())
# create a pivot table to examine this new feature
encoded_sale_condition_pivot_table = pd.pivot_table(train, index='enc_sale_condition', values='SalePrice', aggfunc=np.median)
encoded_sale_condition_pivot_table.plot(kind='bar', color='green')
plt.xlabel('Encoded Sale Condition')
plt.ylabel('Sale Price')
plt.xticks(rotation=0)
# plt.show()

# Before we prepare the data for modeling, we need to deal with the missing data. We'll fill the missing values with an
# average value and then assign the results to data. This is a method of interpolation. The DataFrame.interpolate()
# method makes this simple.

# This is a quick and simple method of dealing with missing values, and might not lead to the best performance of the
# model on new data. Handling missing values is an important part of the modeling process, where creativity and insight
# can make a big difference.

data = train.select_dtypes(include=[np.number]).interpolate().dropna()

# check if all of the columns have 0 null values
print("number of null values =", sum(data.isnull().sum() != 0))

##################################################
# Building a linear model

# Let's perform the final steps to prepare our data for modeling. We'll separate the features and the target variable
# for modeling. We will assign the features to X and the target variable to y. We use np.log() as explained above to
# transform the y variable for the model. data.drop([features], axis=1) tells pandas which columns we want to exclude.
# We won't include SalePrice for obvious reasons, and Id is just an index with no relationship to SalePrice

y = np.log(train.SalePrice)
X = data.drop(['SalePrice', 'Id'], axis=1)

# Let's partition the data and start modeling.
# We will use the train_test_split() function from scikit-learn to create a training set and a hold-out set.
# Partitioning the data in this way allows us to evaluate how our model might perform on data that it has never seen
# before. If we train the model on all of the test data, it will be difficult to tell if overfitting has taken place.

# train_test_split() returns four objects:

# X_train is the subset of our features used for training.
# X_test is the subset which will be our 'hold-out' set - what we'll use to test the model.
# y_train is the target variable SalePrice which corresponds to X_train.
# y_test is the target variable SalePrice which corresponds to X_test.

# The first parameter value X denotes the set of predictor data, and y is the target variable. Next, we set
# random_state=42. This provides for reproducible results, since sci-kit learn's train_test_split will randomly partition
# the data. The test_size parameter tells the function what proportion of the data should be in the test partition.
# In this example, about 33% of the data is devoted to the hold-out set.

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)

# Begin modelling. First create a linear regression model and initialize it

lr = linear_model.LinearRegression()

# Next, we need to fit the model. First instantiate the model and next fit the model. Model fitting is a procedure that
# varies for different types of models. Put simply, we are estimating the relationship between our predictors and the
# target variable so we can make accurate predictions on new data.

# We fit the model using X_train and y_train, and we'll score with X_test and y_test. The lr.fit() method will fit the
# linear regression on the features and target variable that we pass.

lr_model = lr.fit(X_train, y_train)

# Evaluate the performance and visualize results
# Now, we want to evaluate the performance of the model.
# Each competition might evaluate the submissions differently. In this competition, Kaggle will evaluate our submission
# using root-mean-squared-error (RMSE). We'll also look at The r-squared value. The r-squared value is a measure of how
# close the data are to the fitted regression line. It takes a value between 0 and 1, 1 meaning that all of the variance
# in the target is explained by the data. In general, a higher r-squared value means a better fit.

# The model.score() method returns the r-squared value by default.

print("linear regression model's R^2 is ", lr_model.score(X_test, y_test))

# This means that our features explain approximately 89% of the variance in our target variable.

# Next, we'll consider rmse. To do so, use the model we have built to make predictions on the test data set.

predictions = lr_model.predict(X_test)

# The model.predict() method will return a list of predictions given a set of predictors. Use model.predict() after
# fitting the model.

# The mean_squared_error function takes two arrays and calculates the rmse.

rmse = mean_squared_error(y_test, predictions)
print("RMSE of linear model is : ", rmse)

# Interpreting this value is somewhat more intuitive that the r-squared value. The RMSE measures the distance between
# our predicted values and actual values.

# We can view this relationship graphically with a scatter plot.

actual_values = y_test
plt.scatter(predictions, actual_values, alpha=.75,
            color='b') #alpha helps to show overlapping data
plt.xlabel('Predicted Price')
plt.ylabel('Actual Price')
plt.title('Linear Regression Model')
# plt.show()

# If our predicted values were identical to the actual values, this graph would be the straight line y=x because each
# predicted value x would be equal to each actual value y.

# Try to improve the model
# We'll next try using Ridge Regularization to decrease the influence of less important features. Ridge Regularization
# is a process which shrinks the regression coefficients of less important features.

# We'll once again instantiate the model. The Ridge Regularization model takes a parameter, alpha , which controls the
# strength of the regularization.

# We'll experiment by looping through a few different values of alpha, and see how this changes our results.

for i in range(-2, 3):
    alpha = 10**i
    rm = linear_model.Ridge(alpha=alpha)
    ridge_model = rm.fit(X_train, y_train)
    predictions_ridge = ridge_model.predict(X_test)

    plt.scatter(predictions_ridge, actual_values, alpha=0.75, color='b')
    plt.xlabel('Predict Prices')
    plt.ylabel('Actual Prices')
    plt.title('Ridge Regression with alpha = {}'.format(alpha))
    overlay = 'R^2 is {} \n RMSE is {}'.format(ridge_model.score(X_test, y_test), mean_squared_error(y_test, predictions_ridge))
    plt.annotate(s=overlay, xy=(12.1, 10.6), size='x-large')
    plt.show()


# These models perform almost identically to the first model. In our case, adjusting the alpha did not substantially
# improve our model. As you add more features, regularization can be helpful. Repeat this step after you've added more features.
